Hadoop Illuminated Exercise 2:

Introduction:

Welcome to Exercise 2!  In the first lab you learned how to ensure the
five hadoop daemons are running on your pseudo-distributed development
system, and examine the

Lab Objectives:

1. Understand how to view the contents of HDFS
2. Learn how to perform basic file manipulation of your HDFS.
3. Add some files to a directory to prepare for the next exercise.

Step 1) View Files in HDFS
$ hadoop fs -ls

If you do have some data, you will see something like this

$ hadoop fs -ls
Found 1 items
drwxr-xr-x   - hduser supergroup          0 2013-02-19 12:00 /user/um

Otherwise if you have no files you will see nothing.

Step 2) View files in root filestystem
$ hadoop fs -ls /
Found 4 items
drwxr-xr-x   - hduser supergroup          0 2013-02-19 12:00 /app
drwxr-xr-x   - hduser supergroup          0 2013-02-09 21:37 /hbase
drwxr-xr-x   - hduser supergroup          0 2013-02-19 12:00 /tmp
drwxrwxr-x   - hduser supergroup          0 2013-02-19 11:37 /user


STEP 3) create your home directory in HDFS
    $ hadoop dfs -mkdir <your name>
e.g $ hadoop dfs -mkdir tim

This directory may be located in /user/um/<your name>
(um is the login name for the cluster)


STEP 4) Copying files to HDFS
We will copy this README file into HDFS
    $ cd  HadoopIlluminatedSource
    $ hadoop dfs -put hdfs-intro/README.txt  <user name>/
    $ hadoop dfs -ls  <user name>

output might look like:
rwxr-xr-x   - hduser supergroup          0 2013-02-19 12:00 README.txt

see the contents of this file from HDFS
    $ hadoop dfs -cat <yourname>/README.txt
This should print out the file README.txt


STEP 5) Check that python or ruby is running on your system
    $ python --version
    $ ruby --version


STEP 6) Run the python (or ruby) scripts provided to generate some output.
    $ cd HadoopIlluminatedSource  (project root dir)
    $ python scripts/gen-billing-data.py
OR
    $ ruby scripts/gen-billing-data.rb

This will generate a bunch of log files in the current dir


STEP 7) make a directory in hdfs
    $ hadoop dfs -mkdir <your name>/billing/in


STEP 8) Copy the log files generated by the script into hdfs
    $ hadoop dfs -put *.log <your name>/billing/in

Also copy the sample data file also
    $ cd  HadoopIllumiantedSource  (project root)
    $ hadoop dfs -put data/billing-data/sample.txt   <your name>/billing/in
verify the files are there
    $ hadoop dfs -ls <your name>/billing/in



BONUS LAB:

Step 9) copy the files back to your local directory
    $ hadoop dfs -get <your name>/billing/in ./billing_data2






