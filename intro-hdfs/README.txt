Hadoop Illuminated Exercise 2: 

Introduction:

Welcome to Exercise 2!  In the first lab you learned how to ensure the
five hadoop daemons are running on your pseudo-distributed development
system, and examine the 

Lab Objectives:

1. Understand how to view the contents of HDFS
2. Learn how to perform basic file manipulation of your HDFS.
3. Add some files to a directory to prepare for the next exercise.

Step 1) View Files in HDFS
$ hadoop fs -ls

If you do have some data, you will see something like this

$ hadoop fs -ls
Found 1 items
drwxr-xr-x   - hduser supergroup          0 2013-02-19 12:00 /user/hduser/xyz

Otherwise if you have no files you will see nothing.

Step 2) View files in root filestystem
$ hadoop fs -ls /
Found 4 items
drwxr-xr-x   - hduser supergroup          0 2013-02-19 12:00 /app
drwxr-xr-x   - hduser supergroup          0 2013-02-09 21:37 /hbase
drwxr-xr-x   - hduser supergroup          0 2013-02-19 12:00 /tmp
drwxrwxr-x   - hduser supergroup          0 2013-02-19 11:37 /user

Step 3) Copying files to HDFS

Create a small file test.txt.  Copy the file using the following command:
$ hadoop fs -copyFromLocal test.txt
$ hadoop fs -ls 
rwxr-xr-x   - hduser supergroup          0 2013-02-19 12:00 test.txt

$ hadoop fs -cat test.txt
This should print out the file test.txt

Step 4) Check that python or ruby is running on your system
$ python --version
$ ruby --version

Step 5) Run the python (or ruby) scripts provided to generate some output.
$ python scripts/gen-billing-data.py
OR
$ ruby scripts/gen-billing-data.rb

Step 6) make a directory in hdfs
$ hadoop fs -mkdir billing-data

Step 7) Copy the log files generated by the script into hdfs
$ hadoop fs -copyFromLocal *.log billing-data

BONUS:

Step 8) copy the files back to your local directory
$ hadoop fs -copyToLocal billing-data ./billing_data2






